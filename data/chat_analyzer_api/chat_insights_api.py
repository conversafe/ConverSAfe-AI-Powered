# -*- coding: utf-8 -*-
"""chat_insights_api.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WUeeKHzT9TMvm0v2cdLdZ2P6srX1rDa3
"""

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import google.generativeai as genai
import json
from typing import List
import os
from dotenv import load_dotenv
from fastapi.middleware.cors import CORSMiddleware

# Cargar variables de entorno desde .env
load_dotenv()

# Inicializaci√≥n de FastAPI
app = FastAPI()

# Cargar y parsear los or√≠genes desde env
raw_origins = os.getenv("CORS_ALLOWED_ORIGINS", "")
origins = [origin.strip() for origin in raw_origins.split(",") if origin.strip()]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Configuraci√≥n segura de la API de Gemini
api_key = os.getenv("GEMINI_API_KEY")
if not api_key:
    print("‚ùå GEMINI_API_KEY no encontrada en variables de entorno")
    print("üìÅ Aseg√∫rate de tener un archivo .env con: GEMINI_API_KEY=tu_clave_aqui")
    print("üìç Ubicaciones buscadas: ./data/.env, ./data/chat_analyzer_api/.env")
    raise ValueError("GEMINI_API_KEY no encontrada en variables de entorno")
else:
    print("‚úÖ GEMINI_API_KEY cargada correctamente")

genai.configure(api_key=api_key)
model = genai.GenerativeModel("gemini-1.5-flash")


# Modelos de entrada en snake_case
class Mensaje(BaseModel):
    usuario: str
    email: str
    roomId: str
    texto: str
    marca_de_tiempo: str


class Conversacion(BaseModel):
    mensajes: List[Mensaje]


@app.get("/")
def read_root():
    return {"status": "Chat Analysis Service is running"}


@app.post("/analizar_conversacion")
def analizar_conversacion(conversacion: Conversacion):
    try:
        # Preparar conversaci√≥n en formato texto
        mensajes_json = [mensaje.dict() for mensaje in conversacion.mensajes]

        conversation_text = ""
        for msg in mensajes_json:
            conversation_text += (
                f"[{msg['usuario']}]: {msg['texto']} ({msg['marca_de_tiempo']})\n"
            )

        prompt = f"""
        Eres un asistente de IA que analiza la comunicaci√≥n en equipos de desarrollo.

        Conceptos a detectar:
        1. Bloqueadores t√©cnicos.
        2. Cuellos de botella.
        3. Riesgos del proyecto.
        4. Decisiones clave.
        5. Requisitos cambiantes.
        6. Hitos y plazos.
        7. Necesidades de recursos.
        8. Dependencias entre tareas.

        Restricciones:
        - No inventes informaci√≥n.
        - No des opiniones.
        - Arrays vac√≠os deben ser [].
        - Usa snake_case.
        - No uses saltos de l√≠nea innecesarios.
        - Responde SOLO con el JSON, sin texto adicional.
        - El resumen general no debe exceder 50 palabras.
        - Feedback breve: una oraci√≥n por concepto.

        Para cada usuario:
        - nombre: nombre del usuario.
        - participacion: porcentaje de participaci√≥n como n√∫mero entero (ej: 30).
        - eficacia_comunicacion: "alta", "media" o "baja".
        - comentario_eficacia: breve (ej: "Alex es moderadamente claro.").
        - enfoque_conversacion: "enfocado" o "desviado".
        - comentario_enfoque: breve (ej: "Se ci√±e al tema cuando participa.").

        Para el equipo:
        - nivel_conflicto_colaboracion: porcentaje num√©rico (ej: 75 para 75% colaboraci√≥n).
        - estado_actual: "conflictivo" o "colaborador".
        - rendimiento_laboral: porcentaje num√©rico (ej: 80).
        - valoracion_rendimiento: "Bajo", "Medio" o "Alto".
        - actividad:
            - miembros_activos: cantidad de usuarios que participaron.
            - mensajes_de_hoy: cantidad total de mensajes.
            - valoracion_actividad: "Actividad baja", "Actividad media", "Actividad alta".

        Decisiones pendientes:
        - Lista con decisiones mencionadas pero no resueltas.
        - Ejemplos: "Confirmaci√≥n de la l√≠nea de tiempo", "Cuello de botella en la entrega de paquetes", "Asignar a un responsable para el frontend".

        Formato de respuesta:
        ```json
        {{
            "usuarios": [
                {{
                    "nombre": "string",
                    "participacion": 0,
                    "eficacia_comunicacion": "string",
                    "comentario_eficacia": "string",
                    "enfoque_conversacion": "string",
                    "comentario_enfoque": "string"
                }}
            ],
            "equipo": {{
                "nivel_conflicto_colaboracion": 0,
                "estado_actual": "string",
                "rendimiento_laboral": 0,
                "valoracion_rendimiento": "string",
                "actividad": {{
                    "miembros_activos": 0,
                    "mensajes_de_hoy": 0,
                    "valoracion_actividad": "string"
                }}
            }},
            "decisiones_pendientes": [
                "string"
            ],
            "conceptos_detectados": {{
                "bloqueadores_tecnicos": ["string"],
                "cuellos_de_botella": ["string"],
                "riesgos_proyecto": ["string"],
                "decisiones_clave": ["string"],
                "requisitos_cambiantes": ["string"],
                "hitos_plazos": ["string"],
                "necesidades_recursos": ["string"],
                "dependencias_tareas": ["string"]
            }},
            "feedback_asistente": [
                {{
                    "tipo_concepto": "string",
                    "mensaje": "string"
                }}
            ],
            "resumen_general": "string"
        }}
        ```

        Conversaci√≥n a analizar:
        {conversation_text}
        """

        # Llamada a Gemini
        response = model.generate_content(prompt)
        response_text = response.text.strip()

        # Extracci√≥n del JSON
        if "```json" in response_text:
            start_idx = response_text.find("```json") + len("```json")
            end_idx = response_text.rfind("```")
            json_str = response_text[start_idx:end_idx].strip()
        else:
            # Si no hay delimitadores, la respuesta entera es el JSON
            json_str = response_text

        # Validaci√≥n b√°sica de formato JSON
        try:
            resultado_analisis = json.loads(json_str)
        except json.JSONDecodeError:
            raise HTTPException(
                status_code=500, detail="El modelo no devolvi√≥ un JSON v√°lido."
            )

        # NOTA PARA BACKEND:
        #    Guardar el resultado en base de datos.
        #     conexi√≥n:
        #       - Atributos de Usuarios y equipo
        #       - Conceptos para la lista del dashboard
        #       - Conceptos para el feedback
        #    Los datos Importantes est√°n escritos en espa√±ol

        return resultado_analisis

    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Error al procesar la conversaci√≥n: {str(e)}"
        )


@app.post("/analizar_mensaje")
def analizar_mensaje(mensaje: Mensaje):
    try:
        # Construir texto con un solo mensaje
        texto_formateado = (
            f"[{mensaje.usuario} - {mensaje.email}] en sala {mensaje.roomId} "
            f"({mensaje.marca_de_tiempo}): {mensaje.texto}"
        )

        prompt = f"""
        Eres un asistente de IA que analiza comunicaci√≥n en equipos de desarrollo, a partir de un solo mensaje.

        Tu tarea es identificar si el mensaje contiene alguno de los siguientes conceptos:
        1. Bloqueadores t√©cnicos
        2. Cuellos de botella
        3. Riesgos del proyecto
        4. Decisiones clave
        5. Requisitos cambiantes
        6. Hitos y plazos
        7. Necesidades de recursos
        8. Dependencias entre tareas

        Reglas:
        - No inventes informaci√≥n.
        - Si el mensaje no contiene ninguno de los conceptos, responde con arrays vac√≠os.
        - Usa snake_case.
        - No devuelvas texto fuera del JSON.
        - Responde con una estructura que indique el concepto detectado, el nivel de urgencia (alto, medio, bajo) y un mensaje breve.

        Formato de respuesta:
        ```json
        {{
            "usuario": {{
                "nombre": "{mensaje.usuario}",
                "email": "{mensaje.email}",
                "room_id": "{mensaje.roomId}"
            }},
            "conceptos_detectados": {{
                "bloqueadores_tecnicos": ["string"],
                "cuellos_de_botella": ["string"],
                "riesgos_proyecto": ["string"],
                "decisiones_clave": ["string"],
                "requisitos_cambiantes": ["string"],
                "hitos_plazos": ["string"],
                "necesidades_recursos": ["string"],
                "dependencias_tareas": ["string"]
            }},
            "nivel_urgencia": "alto" | "medio" | "bajo",
            "comentario_asistente": "string"
        }}
        ```

        Mensaje:
        {texto_formateado}
        """

        # Llamar al modelo
        response = model.generate_content(prompt)
        response_text = response.text.strip()

        # Parsear JSON devuelto por Gemini
        if "```json" in response_text:
            start = response_text.find("```json") + len("```json")
            end = response_text.rfind("```")
            json_str = response_text[start:end].strip()
        else:
            json_str = response_text

        try:
            resultado = json.loads(json_str)
        except json.JSONDecodeError:
            raise HTTPException(
                status_code=500, detail="JSON inv√°lido devuelto por la IA"
            )

        return resultado

    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Error al analizar el mensaje: {str(e)}"
        )
